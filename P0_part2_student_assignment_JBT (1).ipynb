{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cddFHyPuhhtu",
      "metadata": {
        "id": "cddFHyPuhhtu"
      },
      "source": [
        "# P0: Simple models with Keras\n",
        "\n",
        "**Goal**: implement **three models** for multiclass text classification on the [Women's E-commerce clothing reviews](https://github.com/ya-stack/Women-s-Ecommerce-Clothing-Reviews) dataset, two of them simple feed-forward models using a `Tokenizer` and `TextVectorizer`, respectively, and the third a Convolutional Neural Network (CNN) using a `TextVectorizer` layer and embeddings.\n",
        "\n",
        "**Teams**: one person or two.\n",
        "\n",
        "**Due date**: Check virtual campus.\n",
        "\n",
        "\n",
        "### 1. Data preparation\n",
        "\n",
        "The first step is to downlad the dataset (a `csv` file) from *GitHub*. Suggestions:\n",
        "- You can use the utility function [`tensorflow.keras.utils.get_file()`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file) to download the file. You should set an absolute path to save the file, taking into account that, in *Google Colaboratory*, you have direct acces to the folder `/content/`.\n",
        "- There are many ways to load a `csv` in memory. One simple way is to use `csv.reader()`.\n",
        "\n",
        "~~~\n",
        "with open(path, newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "~~~\n",
        "\n",
        "The resulting data estructure (`data`) is a python list of lists (the reviews)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "IM9whxz-1nUq",
      "metadata": {
        "id": "IM9whxz-1nUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc07387-128a-4d7f-ae1f-43f740fcca1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ruta del archivo que hemos descargado: /content/datasets/Womens_Clothing_E-Commerce_Reviews.csv\n",
            "Tenemos las siguientes filas: 23487\n",
            "Los encabezados de columna son: ['', 'Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import csv\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "DATA_URL = \"https://raw.githubusercontent.com/ya-stack/Women-s-Ecommerce-Clothing-Reviews/master/Womens%20Clothing%20E-Commerce%20Reviews.csv\"\n",
        "FILE_NAME = \"Womens_Clothing_E-Commerce_Reviews.csv\"\n",
        "\n",
        "DOWNLOAD_DIR = \"/content/datasets\"\n",
        "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
        "\n",
        "csv_path = get_file(\n",
        "    fname=FILE_NAME,\n",
        "    origin=DATA_URL,\n",
        "    cache_dir=DOWNLOAD_DIR,\n",
        "    cache_subdir=\"\",\n",
        "    extract=False\n",
        ")\n",
        "\n",
        "with open(csv_path, newline='', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "print(f\"Ruta del archivo que hemos descargado: {csv_path}\")\n",
        "print(f\"Tenemos las siguientes filas: {len(data)}\")\n",
        "print(\"Los encabezados de columna son:\", data[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9227adb7",
      "metadata": {
        "id": "9227adb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6721c58-fef3-4443-fe6e-83879e24e02e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['', 'Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name'], ['0', '767', '33', '', 'Absolutely wonderful - silky and sexy and comfortable', '4', '1', '0', 'Initmates', 'Intimate', 'Intimates'], ['1', '1080', '34', '', 'Love this dress!  it\\'s sooo pretty.  i happened to find it in a store, and i\\'m glad i did bc i never would have ordered it online bc it\\'s petite.  i bought a petite and am 5\\'8\".  i love the length on me- hits just a little below the knee.  would definitely be a true midi on someone who is truly petite.', '5', '1', '4', 'General', 'Dresses', 'Dresses'], ['2', '1077', '60', 'Some major design flaws', 'I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium, which was just ok. overall, the top half was comfortable and fit nicely, but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo, a major design flaw was the net over layer sewn directly into the zipper - it c', '3', '0', '0', 'General', 'Dresses', 'Dresses'], ['3', '1049', '50', 'My favorite buy!', \"I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!\", '5', '1', '0', 'General Petite', 'Bottoms', 'Pants']]\n"
          ]
        }
      ],
      "source": [
        "#Vemos qué pinta tienen los datos\n",
        "print(data[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DsObjWyPkQo0",
      "metadata": {
        "id": "DsObjWyPkQo0"
      },
      "source": [
        "Once you have the rows of the `csv` file in a data structure (remember that the first one is the names of the attributes of the data set, and must be discarded) you have to preprocess the data for its use as an input to the neural networks:\n",
        "1. Extract the textual data from the rows, included in the fields `Title` and `Review Text`, and join both fields if title is not empty.\n",
        "2. Convert the field `Rating`, whose content are integers in the interval [1,5] into three classes: negative (ratings 1,2), neutral (rating 3) and positive (ratings 4,5).\n",
        "3. The dataset contains about 23,000 reviwes. Reserve the first 18,000 for training, and the rest for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "JOIPjJMKhcbt",
      "metadata": {
        "id": "JOIPjJMKhcbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b6e2818-a25a-4747-c36d-d3ec88072256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reseñas: 23486\n",
            "Entrenamos con 18000 reseñas\n",
            "Validamos con 5486 reseñas\n",
            "['Absolutely wonderful - silky and sexy and comfortable', 'Love this dress!  it\\'s sooo pretty.  i happened to find it in a store, and i\\'m glad i did bc i never would have ordered it online bc it\\'s petite.  i bought a petite and am 5\\'8\".  i love the length on me- hits just a little below the knee.  would definitely be a true midi on someone who is truly petite.', 'Some major design flaws. I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium, which was just ok. overall, the top half was comfortable and fit nicely, but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo, a major design flaw was the net over layer sewn directly into the zipper - it c', \"My favorite buy!. I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!\", 'Flattering shirt. This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!']\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "\n",
        "# Separamos la primera fila con los encabezados de columna\n",
        "header, *rows = data\n",
        "\n",
        "def juntar_titulo_y_reseña(title: str, review: str) -> str:\n",
        "\n",
        "    title = title.strip()\n",
        "    review = review.strip()\n",
        "    if title and review:\n",
        "        return f\"{title}. {review}\"\n",
        "    if title:\n",
        "        return title\n",
        "    return review\n",
        "\n",
        "def rating_agrupa(rating_str: str) -> int:\n",
        "\n",
        "    rating = int(rating_str)\n",
        "    if rating <= 2:\n",
        "        return 0  # clase 0: reseñas negativas\n",
        "    if rating == 3:\n",
        "        return 1  # clase 1: reseñas neutrales\n",
        "    return 2      # clase 2: reseñas positivas\n",
        "\n",
        "texts: List[str] = []\n",
        "labels: List[int] = []\n",
        "\n",
        "for row in rows:\n",
        "    title = row[3]\n",
        "    review_text = row[4]\n",
        "    rating_str = row[5]\n",
        "\n",
        "    combined_text = juntar_titulo_y_reseña(title, review_text)\n",
        "    texts.append(combined_text)\n",
        "    labels.append(rating_agrupa(rating_str))\n",
        "\n",
        "\n",
        "train_texts, val_texts = texts[:18000], texts[18000:]\n",
        "train_labels, val_labels = labels[:18000], labels[18000:]\n",
        "\n",
        "print(f\"Reseñas: {len(texts)}\")\n",
        "print(f\"Entrenamos con {len(train_texts)} reseñas\")\n",
        "print(f\"Validamos con {len(val_texts)} reseñas\")\n",
        "print(train_texts[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MeRFaU6MnCpy",
      "metadata": {
        "id": "MeRFaU6MnCpy"
      },
      "source": [
        "### 2. Perceptron with Tokenizer.\n",
        "\n",
        "In the first model, you are going to use a `Tokenizer()` object to process the training and validation texts, transforming each review into binary vectors (of length *n*, where *n* is the size of the vocabulary) in which the positions of the words appearing in the review will be coded as `1` (clue: you can use the method `texts_to_matrix()` for this). You can set a maximum size for the vocabulary (parameter `num_words`), but it is not necessary.\n",
        "\n",
        "Remember that you have to use the `fit_on_texts()` method in order to build the vocabulary of the tokenizer from the training data.\n",
        "\n",
        "In addition, you have to convert vectors with the labels (negative=0, neutral=1, positive=2) from the training and validation sets to a data type which make possible to use them with the loss function `categorical_crossentropy` (clue: you may want to use the utility function `tensorflow.keras.utils.to_categorical()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "z4XZoZSBM6JQ",
      "metadata": {
        "id": "z4XZoZSBM6JQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31618d39-0863-4b31-cd43-2f5c124b37b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape x_train: (18000, 20000)\n",
            "Shape x_val: (5486, 20000)\n",
            "Primeras filas y_train:\n",
            " [[0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Definimos un tamaño máximo de vocabulario (aunque no sea necesario)\n",
        "VOCAB_SIZE = 20000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)  # Aprendemos el vocabulario del set de entrenamiento\n",
        "\n",
        "# Generamos las matrices binarias como en el ejemplo del tokenizer del otro notebook\n",
        "x_train = tokenizer.texts_to_matrix(train_texts, mode=\"binary\")\n",
        "x_val = tokenizer.texts_to_matrix(val_texts, mode=\"binary\")\n",
        "\n",
        "# Convertimos las etiquetas en vectores one-hot\n",
        "y_train = to_categorical(train_labels, num_classes=3)\n",
        "y_val = to_categorical(val_labels, num_classes=3)\n",
        "\n",
        "print(\"Shape x_train:\", x_train.shape)\n",
        "print(\"Shape x_val:\", x_val.shape)\n",
        "print(\"Primeras filas y_train:\\n\", y_train[:3])\n",
        "print(x_train[0:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nBPuYWUDrx6J",
      "metadata": {
        "id": "nBPuYWUDrx6J"
      },
      "source": [
        "Now it is time to create the `Sequential` architecture of out first model. In this case, a simple perceptron with three layers (input, hidden with relu, output with Softmax) will suffice. A few pointers:\n",
        "- You will need to set the `input_shape` of the first layer of the network to the size of the vocabulary in the `Tokenizer`.\n",
        "- The number of units and the activation function in the output layer must be appropiate for a three-class classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5ec70f68",
      "metadata": {
        "id": "5ec70f68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e610ea92-71e3-4115-c006-8cbbe30c79cf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m1,280,064\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,064</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,280,259\u001b[0m (4.88 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,259</span> (4.88 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,280,259\u001b[0m (4.88 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,259</span> (4.88 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "model = Sequential(\n",
        "    [\n",
        "        Input(shape=(VOCAB_SIZE,)),           # dimensión igual al vocabulario controlado por el Tokenizer\n",
        "        Dense(64, activation=\"relu\"),\n",
        "        Dense(3, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D7zWtsGDsyY2",
      "metadata": {
        "id": "D7zWtsGDsyY2"
      },
      "source": [
        "Now compile and train the model. You can use any optimizer you want, but the loss function must be `categorical_crossentropy`, the metric used will be `accuracy`, and you will provide the validation sets for the computation of the validation loss and validation accuracy at the end of each epoch of training, with the argument `validation_data`.\n",
        "\n",
        "The model will train for 10 epochs.\n",
        "\n",
        "Expect a validation accuracy of 0.80-0.83, approximately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "93458fd7",
      "metadata": {
        "id": "93458fd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6226d5-3dba-442c-f649-b4fc61f29051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 222ms/step - accuracy: 0.7336 - loss: 0.8025 - val_accuracy: 0.7763 - val_loss: 0.5611\n",
            "Epoch 2/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 245ms/step - accuracy: 0.7784 - loss: 0.5003 - val_accuracy: 0.7858 - val_loss: 0.4785\n",
            "Epoch 3/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - accuracy: 0.8031 - loss: 0.4145 - val_accuracy: 0.7906 - val_loss: 0.4570\n",
            "Epoch 4/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 153ms/step - accuracy: 0.8229 - loss: 0.3804 - val_accuracy: 0.8246 - val_loss: 0.4349\n",
            "Epoch 5/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - accuracy: 0.8926 - loss: 0.3101 - val_accuracy: 0.8356 - val_loss: 0.4215\n",
            "Epoch 6/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 0.9146 - loss: 0.2649 - val_accuracy: 0.8347 - val_loss: 0.4243\n",
            "Epoch 7/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 131ms/step - accuracy: 0.9238 - loss: 0.2315 - val_accuracy: 0.8332 - val_loss: 0.4347\n",
            "Epoch 8/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.9359 - loss: 0.2017 - val_accuracy: 0.8314 - val_loss: 0.4482\n",
            "Epoch 9/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - accuracy: 0.9428 - loss: 0.1810 - val_accuracy: 0.8270 - val_loss: 0.4614\n",
            "Epoch 10/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 118ms/step - accuracy: 0.9488 - loss: 0.1608 - val_accuracy: 0.8277 - val_loss: 0.4832\n"
          ]
        }
      ],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=512,\n",
        "    validation_data=(x_val, y_val),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yd_GU74Qt00D",
      "metadata": {
        "id": "yd_GU74Qt00D"
      },
      "source": [
        "¿Does the validation accuracy grow with each epoch?\n",
        "\n",
        "For the 10 epochs the model is trained, the validation accuracy grow with each. That shows that the model is not overfitted through the training with these 10 epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A_dNVEIevs25",
      "metadata": {
        "id": "A_dNVEIevs25"
      },
      "source": [
        "### 3. Perceptron with a TextVectorizer layer.\n",
        "\n",
        "Now you are going to implement a new neural network, with two differences with respect to the previous one:\n",
        "- We will use a `TextVectorizer` Layer instead of a `Tokenizer`.\n",
        "- The loss function will be `sparse_categorical_crossentropy`.\n",
        "\n",
        "Your first task is to set the `TextVectorization` layer. Remember you have to create the layer and call the method `adapt()` on the training data before adding the layer to the new model. You can use the default values when creating the layer if you wish, except for `output_mode` that has to be set to `'multi_hot'`, so a binary vector the size of the vocabulary is generated for each example, as `Tokenizer` did in the first model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ac19c962",
      "metadata": {
        "id": "ac19c962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac1a8535-81d0-458f-db6c-95ab75ff9286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulario aprendido: 17424\n",
            "Shape x_train_tv: (18000, 17424)\n",
            "Shape x_val_tv: (5486, 17424)\n",
            "tf.Tensor([0 0 0 ... 0 0 0], shape=(17424,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorizer = TextVectorization(output_mode=\"multi_hot\")\n",
        "#tal y como se nos comenta en el enunciado, el output debe ser este, para poder alimentar al perceptrón\n",
        "text_vectorizer.adapt(train_texts)\n",
        "\n",
        "\n",
        "x_train_tv = text_vectorizer(tf.constant(train_texts))\n",
        "x_val_tv = text_vectorizer(tf.constant(val_texts))\n",
        "\n",
        "vocab_size = text_vectorizer.vocabulary_size()\n",
        "print(\"Vocabulario aprendido:\", vocab_size)\n",
        "print(\"Shape x_train_tv:\", x_train_tv.shape)\n",
        "print(\"Shape x_val_tv:\", x_val_tv.shape)\n",
        "print(x_train_tv[0])\n",
        "#Ahora tenemos el conjunto de entrenamiento y de validación como vectores binarios de cada reseña según la aparición de palabras\n",
        "#del vocabulario que ha creado el text_vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O9oKPPNo1Skr",
      "metadata": {
        "id": "O9oKPPNo1Skr"
      },
      "source": [
        "Now you can create your second `Sequential` model, adding its layers one by one. Obviously, the previously created `TextVectorizer` goes first. There is not need to define an input layer. You can add the rest of the layers after the text vectorizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "Geg_avqn1T3E",
      "metadata": {
        "id": "Geg_avqn1T3E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "49c028dd-a9c2-4447-9f63-34c27fa5b12b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization_3            │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m17424\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │     \u001b[38;5;34m1,115,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                 │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization_3            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17424</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,115,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,115,395\u001b[0m (4.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,115,395</span> (4.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,115,395\u001b[0m (4.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,115,395</span> (4.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "second_model = Sequential()\n",
        "second_model.add(text_vectorizer)\n",
        "second_model.add(Dense(64, activation=\"relu\"))\n",
        "second_model.add(Dense(3, activation=\"softmax\"))\n",
        "\n",
        "\n",
        "_ = second_model(tf.constant(train_texts[:1]))\n",
        "\n",
        "second_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mqLJD4p343SP",
      "metadata": {
        "id": "mqLJD4p343SP"
      },
      "source": [
        "Once the topology of the new model is set, you will set the datasets, compile and train it. Important:\n",
        "\n",
        "- Remember that you are supposed to use `sparse_categorical_crossentropy`, so the label vectors for both training and validation will have to be of the appropiate type and dimensions.\n",
        "- `TextVectorizer` will not accept its training (or validation) input as a list of strings. If you are using lists to store your input strings, convert those lists to numpy arrays with np.array().\n",
        "\n",
        "You can use whichever optimizer you prefer, but you will use accuracy to measure the performance of the model, provide the validation data through the argument `validation_data`, and train for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "243a82cc-89e6-4d14-ac46-3526398a0e1c",
      "metadata": {
        "id": "243a82cc-89e6-4d14-ac46-3526398a0e1c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convertimos las listas de Python en arreglos de NumPy\n",
        "# para que Keras genere tensores tf.string.\n",
        "train_texts_np = np.array(train_texts, dtype=object)\n",
        "val_texts_np = np.array(val_texts, dtype=object)\n",
        "\n",
        "# Convertimos las etiquetas a enteros\n",
        "train_labels_np = np.array(train_labels, dtype=\"int64\")\n",
        "val_labels_np = np.array(val_labels, dtype=\"int64\")\n",
        "\n",
        "second_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "LwD3TBL-cFfz",
      "metadata": {
        "id": "LwD3TBL-cFfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "824d29a3-0560-4774-c710-67a7a0d12ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 191ms/step - accuracy: 0.6794 - loss: 0.8624 - val_accuracy: 0.7763 - val_loss: 0.6308\n",
            "Epoch 2/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 159ms/step - accuracy: 0.7740 - loss: 0.5717 - val_accuracy: 0.7814 - val_loss: 0.5139\n",
            "Epoch 3/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 182ms/step - accuracy: 0.8057 - loss: 0.4428 - val_accuracy: 0.8243 - val_loss: 0.4592\n",
            "Epoch 4/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 159ms/step - accuracy: 0.8677 - loss: 0.3669 - val_accuracy: 0.8321 - val_loss: 0.4282\n",
            "Epoch 5/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 190ms/step - accuracy: 0.8919 - loss: 0.3135 - val_accuracy: 0.8356 - val_loss: 0.4203\n",
            "Epoch 6/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 159ms/step - accuracy: 0.9096 - loss: 0.2742 - val_accuracy: 0.8363 - val_loss: 0.4203\n",
            "Epoch 7/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 158ms/step - accuracy: 0.9180 - loss: 0.2430 - val_accuracy: 0.8343 - val_loss: 0.4258\n",
            "Epoch 8/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 188ms/step - accuracy: 0.9324 - loss: 0.2194 - val_accuracy: 0.8327 - val_loss: 0.4348\n",
            "Epoch 9/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - accuracy: 0.9405 - loss: 0.1937 - val_accuracy: 0.8312 - val_loss: 0.4464\n",
            "Epoch 10/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 174ms/step - accuracy: 0.9486 - loss: 0.1748 - val_accuracy: 0.8305 - val_loss: 0.4617\n"
          ]
        }
      ],
      "source": [
        "history_tv = second_model.fit(\n",
        "    train_texts_np,\n",
        "    train_labels_np,\n",
        "    epochs=10,\n",
        "    batch_size=512,\n",
        "    validation_data=(val_texts_np, val_labels_np),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oP13PmGp5c6H",
      "metadata": {
        "id": "oP13PmGp5c6H"
      },
      "source": [
        "¿Is the new model any better than the previous one?\n",
        "\n",
        "(you can write your answer here)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90UD8dU8h_Bt",
      "metadata": {
        "id": "90UD8dU8h_Bt"
      },
      "source": [
        "### 4. CNN with TextVectorizer layer and word embeddings\n",
        "\n",
        "Finally, you are going to train a third model with the following components:\n",
        "- A `TextVectorizer` layer.\n",
        "- An `Embedding` layer.\n",
        "- One or more `Conv1D` layers.\n",
        "- A `GlobalMaxPooling1D` layer.\n",
        "- One or more `Dense` layers for the computation of results.\n",
        "- A output layer with the appropiate activation function for a multiclass classifier.\n",
        "\n",
        "You will use the functional API.\n",
        "\n",
        "Our goal is to process the input texts token by token using a Convolutional Neural Network (CNN) and embeddings. The first step is to define the `TextVectorizer` layer. This time the output of this layer will be a vector of integer numbers (the input for the `Embedding` layer), with one integer for each token in the input text, so `output_mode` must be set to `int` or omitted (since `int` is the default value for this parameter). In addition, all sequences of integers (words) given to the embedding layer must have the same length. To ensure that, you will use the parameter `output_sequence_length` in the definition of the `TextVectorizer` (i.e. `output_sequence_length=100`). That will cut sequences longer than the value of `output_sequence_length` and pad shorter ones with zeros.\n",
        "\n",
        "Once the layer is defined, it will be trained with the method `adapt()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "LYNKH3QRhln2",
      "metadata": {
        "id": "LYNKH3QRhln2"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "cnn_text_vectorizer = layers.TextVectorization(\n",
        "    output_sequence_length=100\n",
        ")\n",
        "cnn_text_vectorizer.adapt(train_texts)\n",
        "\n",
        "vocab_size_cnn = cnn_text_vectorizer.vocabulary_size()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bTzl5vn7jRg-",
      "metadata": {
        "id": "bTzl5vn7jRg-"
      },
      "source": [
        "You have to start the defintion of the model with an `Input` layer, e.g.:\n",
        "\n",
        "~~~\n",
        "inputs = keras.Input(shape=(1,),dtype=tf.string)\n",
        "~~~\n",
        "\n",
        "then you can add the `TextVectorizer`, `Conv1D`, ... layers.\n",
        "\n",
        "The `Embedding` layer has at least two parameters: the size of the vocabulary and the size of the embeddings. For the vocabulary you have two choices: set it in avance when creating the layer, via de `max_tokens` parameter, or to let all tokens of the training set be part of the vocabulary. In the latter case, you can get the vocabulary size from the layer, using the method `vocabulary_size()`.\n",
        "\n",
        "You must the set embedding dimension to a integer value, e.g. `30`.\n",
        "\n",
        "In the `Conv1D` layer you have to set two parameters, `filters` and `kernel_size`. Both are integers. The first can have any integer value (e.g., `64`of `128`) but the higher is set, the bigger the number of computations will be, while the second should be small compared to the length of the sequences of words (e.g. `3` or `5`).\n",
        "\n",
        "We finish with the output layer:\n",
        "\n",
        "~~~\n",
        "outputs = tf.keras.layers.Dense(...)(x)\n",
        "~~~\n",
        "\n",
        "where `x` is the output of the previous layer. At this point we can define the model:\n",
        "\n",
        "~~~\n",
        "model_functional = keras.Model(inputs=inputs, outputs=outputs)\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5DPqbxMwjVpE",
      "metadata": {
        "id": "5DPqbxMwjVpE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "c497fd03-f19a-4f7a-a76a-a0072105477a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"cnn_text_vectorizer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cnn_text_vectorizer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ text_vectorization_4            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │       \u001b[38;5;34m522,750\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m19,328\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ text_vectorization_4            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">522,750</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,328</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m550,529\u001b[0m (2.10 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">550,529</span> (2.10 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m550,529\u001b[0m (2.10 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">550,529</span> (2.10 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "embedding_dim = 30  # Establecemos la dimensión del vector embedding de cada palabra a 30\n",
        "\n",
        "inputs = keras.Input(shape=(1,), dtype=tf.string)\n",
        "x = cnn_text_vectorizer(inputs)\n",
        "x = layers.Embedding(input_dim=vocab_size_cnn, output_dim=embedding_dim)(x)\n",
        "x = layers.Conv1D(filters=128, kernel_size=5, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "outputs = tf.keras.layers.Dense(3, activation=\"softmax\")(x)\n",
        "\n",
        "cnn_model = keras.Model(inputs=inputs, outputs=outputs, name=\"cnn_text_vectorizer\")\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NVUVLS6QoU9Z",
      "metadata": {
        "id": "NVUVLS6QoU9Z"
      },
      "source": [
        "You can use exactly the same datasets than in the previous model for training and validation, and the optimizer of you preference, but you will use accuracy as performance metric and sparse categorical crossentropy as loss function, provide the validation data through the argument `validation_data`, and train the model for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "PinlDFImk4Ma",
      "metadata": {
        "id": "PinlDFImk4Ma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a411b35-a79e-43d6-9d25-b6c2f233c520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 105ms/step - accuracy: 0.7180 - loss: 0.8083 - val_accuracy: 0.7763 - val_loss: 0.6352\n",
            "Epoch 2/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 118ms/step - accuracy: 0.7678 - loss: 0.5983 - val_accuracy: 0.8024 - val_loss: 0.4761\n",
            "Epoch 3/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 115ms/step - accuracy: 0.8189 - loss: 0.4226 - val_accuracy: 0.8263 - val_loss: 0.4224\n",
            "Epoch 4/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 99ms/step - accuracy: 0.8529 - loss: 0.3521 - val_accuracy: 0.8292 - val_loss: 0.4130\n",
            "Epoch 5/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 120ms/step - accuracy: 0.8829 - loss: 0.2939 - val_accuracy: 0.8277 - val_loss: 0.4155\n",
            "Epoch 6/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.9082 - loss: 0.2488 - val_accuracy: 0.8192 - val_loss: 0.4443\n",
            "Epoch 7/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.9254 - loss: 0.2129 - val_accuracy: 0.8241 - val_loss: 0.5006\n",
            "Epoch 8/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 119ms/step - accuracy: 0.9447 - loss: 0.1711 - val_accuracy: 0.8175 - val_loss: 0.5249\n",
            "Epoch 9/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 118ms/step - accuracy: 0.9634 - loss: 0.1309 - val_accuracy: 0.8122 - val_loss: 0.5802\n",
            "Epoch 10/10\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.9784 - loss: 0.0972 - val_accuracy: 0.8113 - val_loss: 0.6338\n"
          ]
        }
      ],
      "source": [
        "# Reutilizamos los datasets de entrenamiento y validación del modelo anterior\n",
        "#La comparación entre los dos últimos modelos depende de la arquitectura\n",
        "cnn_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history_cnn = cnn_model.fit(\n",
        "    train_texts_np,\n",
        "    train_labels_np,\n",
        "    epochs=10,\n",
        "    batch_size=256,\n",
        "    validation_data=(val_texts_np, val_labels_np),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9nI7Zestolma",
      "metadata": {
        "id": "9nI7Zestolma"
      },
      "source": [
        "¿Does the new model perform any better than the previous two?\n",
        "\n",
        "In terms of the validation accuracy, we don't obtain a better peak than in the two previus models. Furthermore, in this model we have the fastest rythm of decreasing of the validation accuracy from the epoch 5 included, so we wouldn't be able to conclude that this model performs better."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}